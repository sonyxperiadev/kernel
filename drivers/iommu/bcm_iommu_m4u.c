/*
 * drivers/iommu/bcm_iommu_m4u.c
 *
 * Copyright (C) 2012 Broadcom, Inc.
 *
 * This software is licensed under the terms of the GNU General Public
 * License version 2, as published by the Free Software Foundation, and
 * may be copied, distributed, and modified under those terms.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 */

#define pr_fmt(fmt) "iommu-m4u: " fmt


#include <linux/module.h>
#include <linux/platform_device.h>
#include <linux/export.h>
#include <linux/interrupt.h>
#include <linux/dma-mapping.h>
#include <linux/slab.h>
#include <linux/io.h>
#include <linux/debugfs.h>
#include <linux/iommu.h>
#ifdef CONFIG_OF
#include <linux/of.h>
#include <linux/of_address.h>
#include <linux/of_irq.h>
#endif /* CONFIG_OF */

#include <mach/sec_api.h>
#include <asm/sizes.h>

#include <plat/bcm_iommu.h>
#include <mach/rdb/brcm_rdb_mmmmu_open.h>

#define	M4U_PAGE_SHIFT			(12)
#define	M4U_TLB_LINE_SHIFT		(3)
#define	M4U_TLB_LINE_SIZE		(1 << M4U_TLB_LINE_SHIFT)

/**
 * Bitmap of the page sizes currently supported
 * The actual pages size used will be actually 1/8th of the page size
 * exported.
 * Page size 4K is an exception. 4K is exported instead of exporting 32K.
 **/
#if 1
#define IOMMU_M4U_PGSIZES	(SZ_4K)
#else
#define IOMMU_M4U_PGSIZES	(SZ_4K | SZ_64K | SZ_128K | SZ_256K \
		| SZ_512K | SZ_1M | SZ_2M | SZ_4M | SZ_8M | SZ_16M \
		| SZ_32M | SZ_64M | SZ_128M | SZ_256M | SZ_512M | SZ_1G)
#endif

#ifdef CONFIG_BCM_IOMMU_DEBUG
#define M4U_ACTIVITY_LIST_SIZE	(4096)
#define M4U_ACTIVITY_PRINT_SIZE	(1024)
#define M4U_PT_DUMP_PRE_SIZE	(64)
#define M4U_PT_DUMP_POST_SIZE	(64)

enum {
	M4U_EVENT_MAP = 1,
	M4U_EVENT_UNMAP,
	M4U_EVENT_INVALIDATE,
	M4U_EVENT_MAX,
};

struct m4u_map {
	unsigned long va;
	phys_addr_t pa;
	size_t len;
};

struct m4u_unmap {
	unsigned long va;
	size_t len;
	int order;
};

struct m4u_invalidate {
	u32 iova;
	int order;
	u32 val;
};

struct m4u_activity_node {
	int	event;
	union {
		struct m4u_map		map;
		struct m4u_unmap	unmap;
		struct m4u_invalidate	invalidate;
	} u;
};
#endif /* CONFIG_BCM_IOMMU_DEBUG */

/**
 * struct m4u_debugfs - defines m4u debugfs entries
 * @dbg_root:	Root dir entry
 * @dbg_xwp_fs:	File entry for exposing error buffer write index
 * @dbg_xmh_fs:	File entry for exposing tlb multi-hit error count
 * @dbg_xrc_fs:	File entry for exposing read error count
 * @dbg_xwc_fs:	File entry for exposing write error count
 * @dbg_xot_fs:	File entry for exposing other error count
 * @dbg_reg_dir:Dir entry for exposing registers
 *
 * @dbg_cr_fs:	File entry for m4u control register
 * @dbg_isr_fs:	File entry for m4u interrupt status register
 * @dbg_imr_fs:	File entry for m4u interrupt mask register
 * @dbg_tbr_fs:	File entry for m4u table base register
 * @dbg_lr_fs:	File entry for m4u limit register
 * @dbg_ldr_fs:	File entry for m4u limit descriptor register
 * @dbg_efl_fs:	File entry for m4u tlb flush register
 * @dbg_elock_fs:	File entry for m4u pte lock register
 * @dbg_eunlock_fs:	File entry for m4u pte unlock register
 * @dbg_xfifo_fs:	File entry for m4u xfifo register
 * @dbg_pccr_fs:	File entry for m4u perf control register
 * @dbg_pcr1_fs:	File entry for m4u perf ctr 1 register
 * @dbg_pcr2_fs:	File entry for m4u perf ctr 2 register
 **/
struct m4u_debugfs {
	struct dentry	*dbg_root;
	struct dentry	*dbg_xwp_fs;
	struct dentry	*dbg_xmh_fs;
	struct dentry	*dbg_xrc_fs;
	struct dentry	*dbg_xwc_fs;
	struct dentry	*dbg_xot_fs;
#ifdef CONFIG_BCM_IOMMU_DEBUG
	struct dentry	*dbg_reg_dir;
	struct dentry	*dbg_cr_fs;
	struct dentry	*dbg_isr_fs;
	struct dentry	*dbg_imr_fs;
	struct dentry	*dbg_tbr_fs;
	struct dentry	*dbg_lr_fs;
	struct dentry	*dbg_ldr_fs;
	struct dentry	*dbg_efl_fs;
	struct dentry	*dbg_elock_fs;
	struct dentry	*dbg_eunlock_fs;
	struct dentry	*dbg_xfifo_fs;
	struct dentry	*dbg_pccr_fs;
	struct dentry	*dbg_pcr1_fs;
	struct dentry	*dbg_pcr2_fs;
#endif
};

/**
 * struct m4u_drvdata - defines m4u driver data
 * @name:	Name of the m4u instance
 * @lock:	lock used to protect adding/removing devices and PT updates
 * @reg_base:	Virtual base address of the m4u instance register space.
 * @irq:	IRQ number of the m4u error/perf interrupt
 * @garbage_page:	Dummy page assigned for accesses beyond limit
 * @pt_handle:	Physical base address of the page table allocated
 * @pt_base:	Virtual base address of the page table.
 * @pt_size:	Page table size (bytes)
 * @xfifo_handle:	Physical base address of the buffer used to collect
 *		debug information generated by m4u h/w.
 * @xfifo_base:	Virtual base address of the m4u debug buffer.
 * @xfifo_size:	Size of the m4u debug buffer (bytes).
 * @xfifo_widx:	The write index ot the m4u debug buffer. Will point to the
 *		next entry in debug buffer where next update will be stored.
 *
 * Allocated when m4u device is probed and initialized with values obtained
 * from platform data - "iommu_platform_data".
 **/
struct m4u_drvdata {
	const char		*name;
	spinlock_t		pgtablelock;
	int			enabled;
	void __iomem		*reg_base;
	int			irq;
	u32			iova_begin;
	u32			iova_end;
	u32			skip_enable;
	struct page		*garbage_page;
	dma_addr_t		pt_handle;
	u32			*pt_base;
	u32			pt_size;
	dma_addr_t		xfifo_handle;
	u32			*xfifo_base;
	u32			xfifo_size;
	u32			xfifo_widx;
	u32			xfifo_rd_err_cnt;
	u32			xfifo_wr_err_cnt;
	u32			xfifo_tlb_mh_cnt;
	u32			xfifo_other_cnt;
#ifdef CONFIG_BCM_IOMMU_DEBUG
	struct m4u_activity_node *activity_list;
	dma_addr_t		activity_list_handle;
	u32			activity_wr_idx;
	spinlock_t		activity_lock;
#endif /* CONFIG_BCM_IOMMU_DEBUG */
	struct m4u_debugfs	debugfs;
};

#ifdef CONFIG_BCM_IOMMU_DEBUG
/* Macros for debugfs creation */
#define _M4U_GET_REG(REG_NAME, REG_KEY)		\
static int get_ ## REG_NAME ##  _val(void *data, u64 *val) \
{ \
	*val = m4u_read_reg(data, MMMMU_OPEN_ ## REG_KEY ## _OFFSET); \
	return 0; \
}

#define _M4U_SET_REG(REG_NAME, REG_KEY)		\
static int set_ ## REG_NAME ##  _val(void *data, u64 val) \
{ \
	m4u_write_reg(data, MMMMU_OPEN_ ## REG_KEY ## _OFFSET, val); \
	return 0; \
}

#define M4U_GET_REG(REG_NAME, REG_KEY)		\
	_M4U_GET_REG(REG_NAME, REG_KEY)		\
	DEFINE_SIMPLE_ATTRIBUTE(m4u_debug_reg_ ## REG_NAME ## _fops,	\
		get_ ## REG_NAME ## _val, NULL, "0x%08llx\n");

#define M4U_SET_REG(REG_NAME, REG_KEY)		\
	_M4U_SET_REG(REG_NAME, REG_KEY)		\
	DEFINE_SIMPLE_ATTRIBUTE(m4u_debug_reg_ ## REG_NAME ## _fops,	\
		NULL, set_ ## REG_NAME ## _val, "0x%08llx\n");

#define M4U_GET_SET_REG(REG_NAME, REG_KEY)	\
	_M4U_GET_REG(REG_NAME, REG_KEY)		\
	_M4U_SET_REG(REG_NAME, REG_KEY)		\
	DEFINE_SIMPLE_ATTRIBUTE(m4u_debug_reg_ ## REG_NAME ## _fops,	\
		get_ ## REG_NAME ## _val, set_ ## REG_NAME ## _val,	\
		"0x%08llx\n");

#define M4U_DEBUGFS_CREATE_REG_FILE(REG_NAME)	\
		mdata->debugfs.dbg_ ## REG_NAME ## _fs =		\
		debugfs_create_file(#REG_NAME, (S_IRUGO|S_IWUSR),	\
		mdata->debugfs.dbg_reg_dir, mdata,			\
		&m4u_debug_reg_ ## REG_NAME ## _fops);

static void m4u_activity_log(struct m4u_drvdata *mdata,
		struct m4u_activity_node *activity)
{
	struct m4u_activity_node *activity_ptr;
	unsigned long flags;

	if (mdata->activity_list) {
		spin_lock_irqsave(&mdata->activity_lock, flags);
		activity_ptr = &mdata->activity_list[mdata->activity_wr_idx++];
		memcpy(activity_ptr, activity, sizeof(*activity));
		if (mdata->activity_wr_idx >= M4U_ACTIVITY_LIST_SIZE)
			mdata->activity_wr_idx = 0;
		spin_unlock_irqrestore(&mdata->activity_lock, flags);
	}
}

static void m4u_activity_dump_single(struct m4u_drvdata *mdata, u32 idx)
{
	struct m4u_activity_node *activity_ptr = &mdata->activity_list[idx];

	switch (activity_ptr->event) {

	case M4U_EVENT_MAP:
		pr_info("[%d]MAP: va(%#lx) pa(%#x) len(%#08x)\n", idx,
				activity_ptr->u.map.va,
				activity_ptr->u.map.pa,
				activity_ptr->u.map.len);
		break;

	case M4U_EVENT_UNMAP:
		pr_info("[%d]UNMAP: va(%#lx) len(%#08x) order(%d)\n", idx,
				activity_ptr->u.unmap.va,
				activity_ptr->u.unmap.len,
				activity_ptr->u.unmap.order);
		break;

	case M4U_EVENT_INVALIDATE:
		pr_info("[%d]INVAL: va(%#x) order(%d) val(%#x)\n", idx,
				       activity_ptr->u.invalidate.iova,
				       activity_ptr->u.invalidate.order,
				       activity_ptr->u.invalidate.val);
		break;

	default:
		break;

	}
}

static void m4u_activity_dump(struct m4u_drvdata *mdata)
{
	unsigned long flags;
	u32 idx;
	u32 i;

	if (mdata->activity_list) {
		spin_lock_irqsave(&mdata->activity_lock, flags);
		if (mdata->activity_wr_idx < M4U_ACTIVITY_PRINT_SIZE)
			idx = M4U_ACTIVITY_LIST_SIZE -
				(M4U_ACTIVITY_PRINT_SIZE -
				 mdata->activity_wr_idx);
		else
			idx = mdata->activity_wr_idx - M4U_ACTIVITY_PRINT_SIZE;

		for (i = 0; i < M4U_ACTIVITY_PRINT_SIZE; i++) {
			m4u_activity_dump_single(mdata, idx);
			idx++;
			if (idx >= M4U_ACTIVITY_LIST_SIZE)
				idx = 0;
		}
		spin_unlock_irqrestore(&mdata->activity_lock, flags);
	}
}

static void m4u_pt_dump(u32 *pt)
{
	u32 i;

	pt -= M4U_PT_DUMP_PRE_SIZE;
	for (i = 0; i < (M4U_PT_DUMP_PRE_SIZE + M4U_PT_DUMP_POST_SIZE); i++) {
		pr_info("pt(%p) pte(%#x)\n",
				pt, *pt);
		pt++;
	}
}
#endif /* CONFIG_BCM_IOMMU_DEBUG */

static inline void m4u_write_reg(struct m4u_drvdata *mdata, u32 offset,
		u32 value)
{
	iowrite32(value, (mdata->reg_base + offset));
}

static inline u32 m4u_read_reg(struct m4u_drvdata *mdata, u32 offset)
{
	return ioread32(mdata->reg_base + offset);
}

static inline u32 m4u_pte(u32 pa, u32 order, u32 valid)
{
	return pa | (order << 8) | (valid & 1);
}

static void m4u_tlb_invalidate(struct m4u_drvdata *mdata, u32 iova,
		int order)
{
#ifdef CONFIG_BCM_IOMMU_DEBUG
	struct m4u_activity_node activity;
#endif
	u32 mask = MMMMU_OPEN_EFL_ADDRESS_MASK;
	u32 flush_order = 0;

	if (order > M4U_TLB_LINE_SHIFT) {
		mask = -1 << (order + M4U_PAGE_SHIFT);
		flush_order = order - M4U_TLB_LINE_SHIFT;
	}
	m4u_write_reg(mdata, MMMMU_OPEN_EFL_OFFSET,
			((iova & mask) | flush_order));
#ifdef CONFIG_BCM_IOMMU_DEBUG
	activity.event = M4U_EVENT_INVALIDATE;
	activity.u.invalidate.iova = iova;
	activity.u.invalidate.order = order;
	activity.u.invalidate.val = (iova & mask) | flush_order;
	m4u_activity_log(mdata, &activity);
	pr_debug("%s iova(%#x) mask(%#x) order(%d) flush_order(%d)\n",
			__func__, iova, mask, order, flush_order);
#endif
}

static int m4u_pt_update(struct m4u_drvdata *mdata, u32 iova, u32 pa,
		int order, u32 valid)
{
	int actual_order;
	int i, j;
	u32 pte;
	u32 *pt_start, *pt;
	unsigned long flags;
	int ret = 0;

	if (iova < mdata->iova_begin) {
		pr_err("%s ERROR: IOVA(%#x) < IOVA_BEGIN(%#x)\n",
				__func__, iova, mdata->iova_begin);
		return -EINVAL;
	}
	pt_start = mdata->pt_base + ((iova - mdata->iova_begin)
			>> M4U_PAGE_SHIFT);

	spin_lock_irqsave(&mdata->pgtablelock, flags);
	if (order == 0) {
		*pt_start = m4u_pte(pa, 0, valid);
	} else if (order >= M4U_TLB_LINE_SHIFT) {
		/* 64K, 128K, ... page size - needs 2D pattern of PTEs
		 * 32K page size doesn't need it. Still, treated so. */
		/* Outer loop for the 8 TLB entries within a CAM
		 * Inner loop for the identical 4K PTE for the order
		 *  = page_size / 32K
		 **/
		actual_order = order - M4U_TLB_LINE_SHIFT;
		for (i = 0; i < (1 << M4U_TLB_LINE_SHIFT); i++) {
			pt = pt_start + i;
			pte = m4u_pte(pa, actual_order, valid);
			for (j = 0; j < (1 << actual_order); j++)  {
				*pt = pte;
				pt += M4U_TLB_LINE_SIZE;
			}
			pa += 1 << (actual_order + M4U_PAGE_SHIFT);
		}
	} else {
		pr_err("M4U mapping request with invalid page order(%d)\n",
				order);
		ret = -EINVAL;
	}
	spin_unlock_irqrestore(&mdata->pgtablelock, flags);
	return ret;
}

static int m4u_pt_clear(struct m4u_drvdata *mdata, u32 iova, int order)
{
	int i;
	u32 *pt;
	unsigned long flags;

	pt = mdata->pt_base + ((iova - mdata->iova_begin)
			>> M4U_PAGE_SHIFT);
	spin_lock_irqsave(&mdata->pgtablelock, flags);
	for (i = 0; i < (1 << order); i++) {
		*pt = *pt & ~1;
		pt++;
	}
	spin_unlock_irqrestore(&mdata->pgtablelock, flags);

	return 1 << (order + M4U_PAGE_SHIFT);
}

static int m4u_reg_init(struct m4u_drvdata *mdata)
{
	int pt_offset;
	u32 tbr, lr, ldr;

	pt_offset = (mdata->iova_begin >> M4U_PAGE_SHIFT);
	tbr = mdata->pt_handle - (pt_offset<<2);
	lr = mdata->iova_end & ~0xFFF;
	ldr = m4u_pte(page_to_phys(mdata->garbage_page), 0, 1);
	m4u_write_reg(mdata, MMMMU_OPEN_CR_OFFSET, 0);
	m4u_write_reg(mdata, MMMMU_OPEN_IMR_OFFSET,
			(MMMMU_OPEN_IMR_EXFIFO_NOT_EMPTY_MASK |
			 MMMMU_OPEN_IMR_PERFCOUNT1_OVERFLOW_MASK |
			 MMMMU_OPEN_IMR_PERFCOUNT2_OVERFLOW_MASK));
	m4u_write_reg(mdata, MMMMU_OPEN_TBR_OFFSET, tbr);
	m4u_write_reg(mdata, MMMMU_OPEN_LR_OFFSET, lr);
	m4u_write_reg(mdata, MMMMU_OPEN_LDR_OFFSET, ldr);

	return 0;
}

static int m4u_enable(struct m4u_drvdata *mdata)
{
	unsigned int m4u_mode = 1;
	int i;
	u32 val;

	if (mdata->enabled) {
		pr_err("M4U already enabled. mdata(%p)\n", mdata);
		return -EEXIST;
	}
	/* Enable M4U via secure service */
	pr_info("Configure M4U in (%d)mode via secure service\n", m4u_mode);
	secure_api_call(SSAPI_BRCM_SET_M4U, m4u_mode, 0, 0, 0);

	/* Invalidate TLB */
	m4u_write_reg(mdata, MMMMU_OPEN_EFL_OFFSET, 0x11);

	/* Clear xfifo entries */
	for (i = 0; i < 8; i++) {
		val = m4u_read_reg(mdata, MMMMU_OPEN_XFIFO_OFFSET);
		if (!(val & MMMMU_OPEN_XFIFO_VALID_MASK))
			break;
	}

	/* Enable xfifo interrupt */
	m4u_write_reg(mdata, MMMMU_OPEN_IMR_OFFSET,
			(MMMMU_OPEN_IMR_PERFCOUNT1_OVERFLOW_MASK |
			 MMMMU_OPEN_IMR_PERFCOUNT2_OVERFLOW_MASK));

	/* Update state machine (debug) */
	mdata->enabled = 1;

	return 0;
}

#ifdef CONFIG_BCM_IOMMU_DEBUG
M4U_GET_REG(cr, CR);
M4U_GET_REG(isr, ISR);
M4U_GET_SET_REG(imr, IMR);
M4U_GET_SET_REG(tbr, TBR);	/* Convert to only get after bringup */
M4U_GET_SET_REG(lr, LR);	/* Convert to only get after bringup */
M4U_GET_REG(ldr, LDR);
M4U_SET_REG(efl, EFL);
M4U_SET_REG(elock, ELOCK);
M4U_SET_REG(eunlock, EUNLOCK);
M4U_GET_REG(xfifo, XFIFO);
M4U_GET_SET_REG(pccr, PCCR);
M4U_GET_SET_REG(pcr1, PCR1);
M4U_GET_SET_REG(pcr2, PCR2);
#endif

static void m4u_debugfs_init(struct m4u_drvdata *mdata)
{
	char debug_name[64];

	/* Create root directory */
	mdata->debugfs.dbg_root = debugfs_create_dir(mdata->name, NULL);
	if (IS_ERR_OR_NULL(mdata->debugfs.dbg_root))
		pr_err("Failed to create debug root dir.\n");

	/* Create debugfs xfifo write pointer file */
	snprintf(debug_name, 64, "xfifo_wr_ptr");
	mdata->debugfs.dbg_xwp_fs = debugfs_create_u32(debug_name, 0664,
			mdata->debugfs.dbg_root,
			(unsigned int *)&mdata->xfifo_widx);

	/* Create debugfs xfifo TLB multi-hit error count file */
	snprintf(debug_name, 64, "xfifo_tlb_mh_cnt");
	mdata->debugfs.dbg_xmh_fs = debugfs_create_u32(debug_name, 0664,
			mdata->debugfs.dbg_root,
			(unsigned int *)&mdata->xfifo_tlb_mh_cnt);

	/* Create debugfs xfifo read error count file */
	snprintf(debug_name, 64, "xfifo_rd_err_cnt");
	mdata->debugfs.dbg_xrc_fs = debugfs_create_u32(debug_name, 0664,
			mdata->debugfs.dbg_root,
			(unsigned int *)&mdata->xfifo_rd_err_cnt);

	/* Create debugfs xfifo write error count file */
	snprintf(debug_name, 64, "xfifo_wr_err_cnt");
	mdata->debugfs.dbg_xwc_fs = debugfs_create_u32(debug_name, 0664,
			mdata->debugfs.dbg_root,
			(unsigned int *)&mdata->xfifo_wr_err_cnt);

	/* Create debugfs xfifo other error count file */
	snprintf(debug_name, 64, "xfifo_other_cnt");
	mdata->debugfs.dbg_xot_fs = debugfs_create_u32(debug_name, 0664,
			mdata->debugfs.dbg_root,
			(unsigned int *)&mdata->xfifo_other_cnt);

#ifdef CONFIG_BCM_IOMMU_DEBUG
	/* Create register files */
	snprintf(debug_name, 64, "registers");
	mdata->debugfs.dbg_reg_dir = debugfs_create_dir(debug_name,
			mdata->debugfs.dbg_root);
	if (IS_ERR_OR_NULL(mdata->debugfs.dbg_reg_dir)) {
		pr_err("Failed to create dir(%s).\n", debug_name);
	} else {
		M4U_DEBUGFS_CREATE_REG_FILE(cr);
		M4U_DEBUGFS_CREATE_REG_FILE(isr);
		M4U_DEBUGFS_CREATE_REG_FILE(imr);
		M4U_DEBUGFS_CREATE_REG_FILE(tbr);
		M4U_DEBUGFS_CREATE_REG_FILE(lr);
		M4U_DEBUGFS_CREATE_REG_FILE(ldr);
		M4U_DEBUGFS_CREATE_REG_FILE(efl);
		M4U_DEBUGFS_CREATE_REG_FILE(elock);
		M4U_DEBUGFS_CREATE_REG_FILE(eunlock);
		M4U_DEBUGFS_CREATE_REG_FILE(xfifo);
		M4U_DEBUGFS_CREATE_REG_FILE(pccr);
		M4U_DEBUGFS_CREATE_REG_FILE(pcr1);
		M4U_DEBUGFS_CREATE_REG_FILE(pcr2);
	}
#endif
}

static int m4u_init(struct m4u_drvdata *mdata)
{
#ifdef CONFIG_BCM_IOMMU_DEBUG
	int activity_buf_size =
		M4U_ACTIVITY_LIST_SIZE * sizeof(struct m4u_activity_node);
#endif /* CONFIG_BCM_IOMMU_DEBUG */

	/* Allocate garbage collection page for limit register */
	mdata->garbage_page = alloc_page(GFP_KERNEL);
	if (mdata->garbage_page == NULL) {
		pr_err("Garbage page allocation failed.\n");
		goto err;
	}
	pr_info("garbage_page(0x%08x)\n", page_to_phys(mdata->garbage_page));

	/* Allocate page table - 4 byte-word per 4K page */
	mdata->pt_base = (u32 *)dma_alloc_coherent(NULL, mdata->pt_size,
			&mdata->pt_handle, (GFP_KERNEL | ___GFP_ZERO));
	if (mdata->pt_base == NULL) {
		pr_err("Page table allocation (0x%x) failed.\n",
				mdata->pt_size);
		goto err;
	}
	pr_info("pt base(0x%08x) size(0x%08x)\n", mdata->pt_handle,
			mdata->pt_size);

	/* Allocate xfifo buffer */
	if (mdata->xfifo_size) {
		mdata->xfifo_base = (u32 *)dma_alloc_coherent(NULL,
				mdata->xfifo_size, &mdata->xfifo_handle,
				(GFP_KERNEL | ___GFP_ZERO));
		if (mdata->xfifo_base == NULL)
			pr_err("xfifo buffer allocation (0x%x) failed.\n",
					mdata->xfifo_size);
		else
			pr_info("xfifo base(0x%08x) size(0x%08x)\n",
					mdata->xfifo_handle, mdata->xfifo_size);
	}

#ifdef CONFIG_BCM_IOMMU_DEBUG
	if (activity_buf_size) {
		mdata->activity_list =
			(struct m4u_activity_node *)dma_alloc_coherent(
					NULL, activity_buf_size,
					&mdata->activity_list_handle,
					(GFP_KERNEL | ___GFP_ZERO));
		if (mdata->activity_list == NULL)
			pr_err("Activity list buffer alloc (0x%x) failed.\n",
					activity_buf_size);
		else
			pr_info("Activity list base(0x%08x) size(0x%08x)\n",
					activity_buf_size,
					mdata->activity_list_handle);
	}
#endif /* CONFIG_BCM_IOMMU_DEBUG */

	/* Initialize m4u registers */
	m4u_reg_init(mdata);
	if (!mdata->skip_enable)
		m4u_enable(mdata);

	return 0;

err:
	pr_err("m4u_init failed\n");
	return -ENOMEM;
}

static void m4u_exit(struct m4u_drvdata *mdata)
{
#ifdef CONFIG_BCM_IOMMU_DEBUG
	int activity_buf_size =
		M4U_ACTIVITY_LIST_SIZE * sizeof(struct m4u_activity_node);

	/* Free activity list buffer if allocated */
	if (mdata->activity_list)
		dma_free_coherent(NULL, activity_buf_size, mdata->activity_list,
				mdata->activity_list_handle);
	mdata->activity_list = NULL;
#endif /* CONFIG_BCM_IOMMU_DEBUG */

	/* Free xfifo buffer if allocated */
	if (mdata->xfifo_base)
		dma_free_coherent(NULL, mdata->xfifo_size, mdata->xfifo_base,
				mdata->xfifo_handle);
	mdata->xfifo_base = NULL;

	/* Free page table if allocated */
	if (mdata->pt_base)
		dma_free_coherent(NULL, mdata->pt_size, mdata->pt_base,
				mdata->pt_handle);
	mdata->pt_base = NULL;

	/* Free garbage collection page if allocated */
	if (mdata->garbage_page)
		__free_page(mdata->garbage_page);
	mdata->garbage_page = NULL;
}

static void m4u_fault_handler(struct m4u_drvdata *mdata, u32 val)
{
	u32 iova = val & ~0xFFF;
	u32 *pt;
	unsigned long flags;

	spin_lock_irqsave(&mdata->pgtablelock, flags);
	pt = mdata->pt_base + ((iova - mdata->iova_begin)
			>> M4U_PAGE_SHIFT);
	if ((val & MMMMU_OPEN_XFIFO_REASON_MASK) == 0) {
		/* Multi-hit TLB error */
		m4u_tlb_invalidate(mdata, 0, 20);
		pr_info("[%04d]xfifo TLB multi hit error(%#x) pt_base(%p) pt(%p) pte(%#x)\n",
				mdata->xfifo_tlb_mh_cnt, val, mdata->pt_base,
				pt, *pt);
		mdata->xfifo_tlb_mh_cnt++;
#ifdef CONFIG_BCM_IOMMU_DEBUG
		m4u_activity_dump(mdata);
		m4u_pt_dump(pt);
#endif
	} else if ((val & MMMMU_OPEN_XFIFO_RW_MASK) == 0) {
		/* Write access to invalid PTE error */
		if ((mdata->xfifo_wr_err_cnt & 0xFFF) == 0)
			pr_info("[%04d]xfifo write error(%#x) pt_base(%p) pt(%p) pte(%#x)\n",
					mdata->xfifo_wr_err_cnt, val,
					mdata->pt_base, pt, *pt);
#ifdef CONFIG_BCM_IOMMU_BROKEN_EXTRA_RW
		if (!(*pt & 1)) {
			*pt = m4u_pte(page_to_phys(mdata->garbage_page), 0, 1);
			m4u_tlb_invalidate(mdata, iova, 0);
		}
#endif
		mdata->xfifo_wr_err_cnt++;
#ifdef CONFIG_BCM_IOMMU_DEBUG
		m4u_activity_dump(mdata);
		m4u_pt_dump(pt);
#endif
	} else if ((val & MMMMU_OPEN_XFIFO_RW_MASK) ==
			MMMMU_OPEN_XFIFO_RW_MASK) {
		/* Read access to invalid PTE error */
		if ((mdata->xfifo_rd_err_cnt & 0xFFF) == 0)
			pr_info("[%04d]xfifo read error(%#x) pt_base(%p) pt(%p) pte(%#x)\n",
					mdata->xfifo_rd_err_cnt, val,
					mdata->pt_base,
					pt, *pt);
#ifdef CONFIG_BCM_IOMMU_BROKEN_EXTRA_RW
		if (!(*pt & 1)) {
			*pt = m4u_pte(page_to_phys(mdata->garbage_page), 0, 1);
			m4u_tlb_invalidate(mdata, iova, 0);
		}
#endif
		mdata->xfifo_rd_err_cnt++;
	} else {
		/* TLB lock, unlock error etc */
		pr_info("[%04d]xfifo Other error(%#x) pt_base(%p) pt(%p) pte(%#x)\n",
				mdata->xfifo_other_cnt, val, mdata->pt_base,
				pt, *pt);
		mdata->xfifo_other_cnt++;
	}
	spin_unlock_irqrestore(&mdata->pgtablelock, flags);

	if (mdata->xfifo_base) {
		mdata->xfifo_base[mdata->xfifo_widx++] = val;
		if (mdata->xfifo_widx >= (mdata->xfifo_size>>2))
			mdata->xfifo_widx = 0;
	}
}

static irqreturn_t m4u_interrupt_handler(int irq, void *dev_id)
{
	struct m4u_drvdata *mdata = (struct m4u_drvdata *)dev_id;
	u32 status = 0;
	u32 val;

	/* Read the status bits */
	status = m4u_read_reg(mdata, MMMMU_OPEN_ISR_OFFSET);
	pr_debug("Interrupt status [0x%08x]\n", status);
	if (status & MMMMU_OPEN_IMR_EXFIFO_NOT_EMPTY_MASK) {
		while (1) {
			val = m4u_read_reg(mdata, MMMMU_OPEN_XFIFO_OFFSET);
			if (val & MMMMU_OPEN_XFIFO_VALID_MASK)
				m4u_fault_handler(mdata, val);
			else
				break;
		}
		return IRQ_HANDLED;
	}
	if (status & MMMMU_OPEN_IMR_PERFCOUNT1_OVERFLOW_MASK) {
		val = m4u_read_reg(mdata, MMMMU_OPEN_PCR1_OFFSET);
		pr_debug("pcr1(0x%x)\n", val);
		/* Mask the iterrupt */
		m4u_write_reg(mdata, MMMMU_OPEN_IMR_OFFSET,
				MMMMU_OPEN_IMR_PERFCOUNT1_OVERFLOW_MASK);
		return IRQ_HANDLED;
	}
	if (status & MMMMU_OPEN_IMR_PERFCOUNT2_OVERFLOW_MASK) {
		val = m4u_read_reg(mdata, MMMMU_OPEN_PCR2_OFFSET);
		pr_debug("pcr2(0x%x)\n", val);
		/* Mask the iterrupt */
		m4u_write_reg(mdata, MMMMU_OPEN_IMR_OFFSET,
				MMMMU_OPEN_IMR_PERFCOUNT2_OVERFLOW_MASK);
		return IRQ_HANDLED;
	}

	return IRQ_NONE;
}

#ifdef CONFIG_OF
#define M4U_OF_READ(_prop_) \
	do { \
		if (of_property_read_u32(node, #_prop_, &val)) { \
			pr_err("ERROR: Prop \"" #_prop_ "\" not found\n"); \
			goto err; \
		} \
		pr_debug(#_prop_ " = %#x\n", val); \
		_prop_ = val; \
	} while (0)

#define M4U_OF_READ_OPT(_prop_) \
	do { \
		if (!of_property_read_u32(node, #_prop_, &val)) { \
			pr_debug(#_prop_ " = %#x\n", val); \
			_prop_ = val; \
		} \
	} while (0)

static u64 m4u_dmamask = DMA_BIT_MASK(32);

static int iommu_m4u_parse_dt(struct device *dev, struct m4u_drvdata *mdata)
{
	struct device_node *node = dev->of_node;
	const char *name;
	u32 val, iova_begin, iova_size, errbuf_size = 0, skip_enable = 0;
	int ret = -EINVAL;

	dev->dma_mask = &m4u_dmamask;
	dev->coherent_dma_mask = DMA_BIT_MASK(32);
	if (of_property_read_string(node, "name", &name)) {
		pr_err("ERROR: Property \"name\" not found\n");
		goto err;
	}
	pr_info("Parsing DT node(%s)\n", name);
	mdata->name = kzalloc(strlen(name) + 1, GFP_KERNEL);
	if (!mdata->name) {
		pr_err("ERROR: Couldn't allocate memory for iommu_name");
		ret = -ENOMEM;
		goto err;
	}
	strcpy((char *)mdata->name, name);
	M4U_OF_READ(iova_begin);
	M4U_OF_READ(iova_size);
	M4U_OF_READ_OPT(errbuf_size);
	M4U_OF_READ_OPT(skip_enable);
	if (!iova_size) {
		pr_err("ERROR: IOVA size given as '0'\n");
		goto err;
	}
	mdata->iova_begin = iova_begin;
	mdata->iova_end = iova_begin + iova_size;
	if ((mdata->iova_begin + iova_size) < iova_size)
		mdata->iova_end = 0xFFFFF000;
	mdata->pt_size = (iova_size >> (M4U_PAGE_SHIFT-2));
	mdata->xfifo_size = errbuf_size;
	mdata->skip_enable = skip_enable;
	return 0;
err:
	return ret;
}
#endif /* CONFIG_OF */

static int iommu_m4u_parse_pdata(struct device *dev, struct m4u_drvdata *mdata)
{
	struct bcm_iommu_pdata *pdata = dev_get_platdata(dev);
	u32 iova_begin, iova_size;
	int ret = -EINVAL;

	mdata->name = pdata->name;
	iova_begin = pdata->iova_begin;
	iova_size = pdata->iova_size;
	if (!iova_size) {
		pr_err("ERROR: IOVA size given as '0'\n");
		goto err;
	}
	mdata->iova_begin = iova_begin;
	mdata->iova_end = iova_begin + iova_size;
	if ((mdata->iova_begin + iova_size) < iova_size)
		mdata->iova_end = 0xFFFFF000;
	mdata->pt_size = (iova_size >> (M4U_PAGE_SHIFT-2));
	mdata->xfifo_size = pdata->errbuf_size;
	mdata->skip_enable = pdata->skip_enable;
	return 0;
err:
	return ret;
}

static int iommu_m4u_remove(struct platform_device *pdev)
{
	struct m4u_drvdata *mdata = platform_get_drvdata(pdev);

	pr_info("M4U device remove\n");
	if (mdata) {
		if (mdata->debugfs.dbg_root)
			debugfs_remove_recursive(mdata->debugfs.dbg_root);
		if (mdata->irq)
			free_irq(mdata->irq, mdata);
		m4u_exit(mdata);
		if (mdata->reg_base)
			iounmap(mdata->reg_base);
		if (pdev->dev.of_node)
			kfree(mdata->name);
	}
	kfree(mdata);
	dev_set_drvdata(&pdev->dev, NULL);
	return 0;
}

static int iommu_m4u_probe(struct platform_device *pdev)
{
	struct m4u_drvdata *mdata;
	struct device *dev = &pdev->dev;
	int ret = -EINVAL;

	/* Alloc device specific context */
	mdata = kzalloc(sizeof(*mdata), GFP_KERNEL);
	if (!mdata) {
		pr_err("Allocation of context failed.\n");
		ret = -ENOMEM;
		goto err;
	}

	/* Set the mdata as platform device private data */
	ret = dev_set_drvdata(dev, mdata);
	if (ret) {
		pr_err("Unabled to initialize driver data\n");
		goto err;
	}

	if (dev_get_platdata(dev)) {
		/* Initialize the driver data with pdata */
		pr_info("Probe: via platform_data\n");
		if (iommu_m4u_parse_pdata(dev, mdata)) {
			pr_err("Probe Fail: pdata parsing failed\n");
			goto err;
		}
#ifdef CONFIG_OF
	} else if (dev->of_node) {
		pr_info("Probe: via DT framework\n");
		if (iommu_m4u_parse_dt(dev, mdata)) {
			pr_err("Probe Fail: DT parsing failed\n");
			goto err;
		}
#endif /* CONFIG_OF */
	} else {
		pr_err("Probe Fail: No platform_data, no DT\n");
		goto err;
	}
	pr_info("iova range(%#08x - %#08x)\n", mdata->iova_begin,
			mdata->iova_end);

	/* Initialize lock variables */
	spin_lock_init(&mdata->pgtablelock);
#ifdef CONFIG_BCM_IOMMU_DEBUG
	spin_lock_init(&mdata->activity_lock);
#endif /* CONFIG_BCM_IOMMU_DEBUG */

	/* Map the register space */
	if (dev_get_platdata(dev)) {
		struct resource *res;
		res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
		if (!res) {
			pr_err("Register base not specified.\n");
			ret = -ENOENT;
			goto err;
		}
		pr_info("reg(%x) size(%x)\n", res->start, resource_size(res));
		mdata->reg_base = ioremap_nocache(res->start,
				resource_size(res));
#ifdef CONFIG_OF
	} else {
		mdata->reg_base = of_iomap(dev->of_node, 0);
#endif
	}
	if (!mdata->reg_base) {
		pr_err("Failed to map the registers.\n");
		ret = -ENOMEM;
		goto err;
	}

	/* Initialize m4u page table and registers as per platform data */
	if (m4u_init(mdata)) {
		pr_err("Failed to initialize m4u.\n");
		ret = -ENOMEM;
		goto err;
	}

	/* Register IRQ */
	if (dev_get_platdata(dev)) {
		mdata->irq = platform_get_irq(pdev, 0);
#ifdef CONFIG_OF
	} else {
		mdata->irq = of_irq_to_resource(dev->of_node, 0, NULL);
#endif
	}
	ret = request_irq(mdata->irq, m4u_interrupt_handler, 0, mdata->name,
			(void *)mdata);
	if (ret) {
		pr_err("Failed to register IRQ (%d).\n", mdata->irq);
		mdata->irq = 0;
		ret = -ENODEV;
		goto err;
	}

	/* Create Debug files */
	m4u_debugfs_init(mdata);

	/* Print the device name, drvdata addr, regbase and irq number */
	pr_info("probe success (%s) - dev(%p) mdata(%#08x) irq(%d) reg-base(%#08x)\n",
			mdata->name, dev, (u32)mdata, mdata->irq,
			(u32)mdata->reg_base);
	return 0;

err:
	/* Error exit */
	iommu_m4u_remove(pdev);
	pr_err("Failed to probe\n");
	return ret;
}

#ifdef CONFIG_PM
static int iommu_m4u_suspend(struct platform_device *pdev, pm_message_t message)
{
	return 0;
}

static int iommu_m4u_resume(struct platform_device *pdev)
{
	struct m4u_drvdata *mdata = dev_get_drvdata(&pdev->dev);

	/* Invalidate entire IOVA range of 4G = 1M pages */
	m4u_tlb_invalidate(mdata, 0, 20);
	return 0;
}

#else

#define	iommu_m4u_suspend	NULL
#define	iommu_m4u_resume	NULL

#endif

#ifdef CONFIG_OF
static const struct of_device_id iommu_m4u_of_match[] = {
	{ .compatible = "bcm,iommu-m4u", },
	{},
};
#else
#define iommu_m4u_of_match NULL
#endif /* CONFIG_OF */

static struct platform_driver iommu_m4u_driver = {
	.probe		= iommu_m4u_probe,
	.remove		= iommu_m4u_remove,
	.driver = {
		.name	= "iommu-m4u-bcm",
		.owner	= THIS_MODULE,
		.of_match_table = iommu_m4u_of_match,
	},
#ifdef CONFIG_PM
	.suspend	= iommu_m4u_suspend,
	.resume		= iommu_m4u_resume,
#endif
};

static int iommu_m4u_domain_init(struct iommu_domain *domain)
{
	pr_debug("domain init(%p)\n", domain);
	return 0;
}

static void iommu_m4u_domain_destroy(struct iommu_domain *domain)
{
	pr_info("domain destroy(%p)\n", domain);
}

static int iommu_m4u_attach_dev(struct iommu_domain *domain, struct device *dev)
{
	struct m4u_drvdata *mdata = dev_get_drvdata(dev->archdata.iommu);
	int ret = 0;

	pr_info("Attach IO device(%p) to iommu dev(%p) domain(%p) mdata(%p)\n",
			dev, dev->archdata.iommu, domain, mdata);
	if (!mdata) {
		pr_err("Dev attach fail: IOMMU device(%p) not set up\n",
				dev->archdata.iommu);
		return 1;
	}
	if (domain->priv && (domain->priv != mdata)) {
		/* If any existing device is attached to domain and mdata does
		 * not match, print error.
		 * Assumption is that one mapping will have only one domain and
		 * one domain will point to only one iommu device and
		 * multiple devices can get attached to domain"
		 **/
		pr_err("Dev attach fail: domain already attached to mdata(%p)\n",
				domain->priv);
		return 1;
	}
	domain->priv = mdata;
	return ret;
}

static void iommu_m4u_detach_dev(struct iommu_domain *domain,
				 struct device *dev)
{
	struct m4u_drvdata *mdata = dev_get_drvdata(dev->archdata.iommu);
	pr_info("Detach IO device(%p) from iommu dev(%p) domain(%p) mdata(%p)\n",
			dev, dev->archdata.iommu, domain, mdata);
}

static int iommu_m4u_map(struct iommu_domain *domain, unsigned long va,
			 phys_addr_t pa, size_t len, int prot)
{
	struct m4u_drvdata *mdata = domain->priv;
	int order;
	int ret = 0;

#ifdef CONFIG_BCM_IOMMU_DEBUG
	struct m4u_activity_node activity;

	activity.event = M4U_EVENT_MAP;
	activity.u.map.va = va;
	activity.u.map.pa = pa;
	activity.u.map.len = len;
	m4u_activity_log(mdata, &activity);
	pr_debug("map va(%#08lx) pa(%#08x) len(%#08x) prot(%x) to iommu(%p) domain(%p)\n",
			va, pa, len, prot, mdata, domain);
#endif
	order = __fls(len) - M4U_PAGE_SHIFT;
	ret = m4u_pt_update(mdata, va, pa, order, 1);
	m4u_tlb_invalidate(mdata, va, order);
	return ret;
}

static int iommu_m4u_map_get_order(unsigned long iova, size_t size)
{
	unsigned int pgsize_idx, align_pgsize_idx;
	unsigned long pgsize;

	/* Max page size that still fits into 'size' */
	pgsize_idx = __fls(size);

	/* Max page size allowed by iova */
	align_pgsize_idx = __ffs(iova);

	pgsize_idx = min(pgsize_idx, align_pgsize_idx);

	/* build a mask of acceptable page sizes */
	pgsize = (1UL << (pgsize_idx + 1)) - 1;

	/* throw away page sizes not supported by the hardware */
	pgsize &= IOMMU_M4U_PGSIZES;

	/* pick the biggest page */
	pgsize_idx = __fls(pgsize);

	return pgsize_idx - M4U_PAGE_SHIFT;
}

static size_t iommu_m4u_unmap(struct iommu_domain *domain, unsigned long va,
			    size_t len)
{
	struct m4u_drvdata *mdata = domain->priv;
#ifdef CONFIG_BCM_IOMMU_DEBUG
	struct m4u_activity_node activity;
#endif
	int order;

	order = iommu_m4u_map_get_order(va, len);
#ifdef CONFIG_BCM_IOMMU_DEBUG
	activity.event = M4U_EVENT_UNMAP;
	activity.u.unmap.va = va;
	activity.u.unmap.len = len;
	activity.u.unmap.order = order;
	m4u_activity_log(mdata, &activity);
	pr_debug("unmap va(%#08lx) len(%#08x) order(%d) from iommu(%p) domain(%p)\n",
			va, len, order, mdata, domain);
#endif
	m4u_pt_clear(mdata, va, order);
	m4u_tlb_invalidate(mdata, va, order);

	/* the IOMMU API requires us to return how many bytes were unmapped */
	return 1 << (order + M4U_PAGE_SHIFT);
}

static phys_addr_t iommu_m4u_iova_to_phys(struct iommu_domain *domain,
					  dma_addr_t iova)
{
	struct m4u_drvdata *mdata = domain->priv;
	phys_addr_t ret = 0;

	pr_info("iova_to_phys: domain(%p) iova(%#08x) mdata(%p)\n",
			domain, iova, mdata);
	/* Cannot do because of the 2D page table format.
	 * API not used by any of the drivers */

	ret = iova;
	return ret;
}

static int iommu_m4u_domain_has_cap(struct iommu_domain *domain,
				    unsigned long cap)
{
	pr_info("query capability cap(%#lx) of domain(%p)\n",
			cap, domain);
	return 0;
}

static struct iommu_ops iommu_m4u_ops = {
	.domain_init = iommu_m4u_domain_init,
	.domain_destroy = iommu_m4u_domain_destroy,
	.attach_dev = iommu_m4u_attach_dev,
	.detach_dev = iommu_m4u_detach_dev,
	.map = iommu_m4u_map,
	.unmap = iommu_m4u_unmap,
	.iova_to_phys = iommu_m4u_iova_to_phys,
	.domain_has_cap = iommu_m4u_domain_has_cap,
	.pgsize_bitmap = IOMMU_M4U_PGSIZES,
};

static int __init iommu_m4u_init(void)
{
	pr_info("%s\n", __func__);
	return platform_driver_register(&iommu_m4u_driver);
}

static void __exit iommu_m4u_exit(void)
{
	pr_info("%s\n", __func__);
	platform_driver_unregister(&iommu_m4u_driver);
}
subsys_initcall(iommu_m4u_init);
module_exit(iommu_m4u_exit);

static int __init iommu_m4u_ops_init(void)
{
	pr_info("Attach m4u ops to platform bus\n");
	return bus_set_iommu(&platform_bus_type, &iommu_m4u_ops);
}
arch_initcall(iommu_m4u_ops_init);

int bcm_iommu_enable(struct device *dev)
{
	struct m4u_drvdata *mdata = dev_get_drvdata(dev->archdata.iommu);

	if (!mdata) {
		pr_err("IOMMU enable fail: IOMMU device(%p) not set up\n",
				dev->archdata.iommu);
		return -ENODEV;
	}
	return m4u_enable(mdata);
}
EXPORT_SYMBOL(bcm_iommu_enable);

MODULE_AUTHOR("Nishanth Peethambaran <nishanth@broadcom.com>");
MODULE_DESCRIPTION("M4U device driver");
MODULE_LICENSE("GPL v2");
